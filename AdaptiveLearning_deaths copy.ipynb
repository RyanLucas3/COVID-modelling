{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Adaptive Learning: Deaths**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as sm\n",
    "from itertools import product\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "class ModelGroupSpecs:\n",
    "    def __init__(self, ar_orders, desired_model_groups):\n",
    "        self.ar_orders = ar_orders\n",
    "        self.desired_model_groups = desired_model_groups\n",
    "\n",
    "    def get_all_possible_combinations(self, model_group, MG_ar_orders, MG_regressors):\n",
    "        return list(product(model_group, MG_ar_orders, MG_regressors))\n",
    "\n",
    "    def create_functional_sets(self):\n",
    "        output = []\n",
    "        if \"AR\" in self.desired_model_groups:\n",
    "            # MG1 = Model Group 1: AR models.\n",
    "\n",
    "            self.AR_models = self.get_all_possible_combinations(\n",
    "                model_group=['AR'],\n",
    "                MG_ar_orders=self.ar_orders,\n",
    "                MG_regressors=[None])\n",
    "            output.append(self.AR_models)\n",
    "\n",
    "        if \"OLS\" in self.desired_model_groups:\n",
    "            # MG2N = Model Group 2N: Single Variable Exogenous OLS\n",
    "\n",
    "            self.OLS_models = self.get_all_possible_combinations(\n",
    "                model_group=['OLS'],\n",
    "                MG_ar_orders=[None],#'log_new_vaccines_per_capita\t', \n",
    "                MG_regressors=['log_new_people_vaccinated_per_capita','delta_cases_per_capita',\n",
    "                                'delta_cases_per_capita_United_Kingdom', 'delta_cases_per_capita_Germany', 'delta_cases_per_capita_France',\n",
    "                                'delta_deaths_per_capita_United Kingdom', 'delta_deaths_per_capita_Germany', 'delta_deaths_per_capita_France',\n",
    "                                'full_lockdown', 'full_lockdown.l30', 'full_lockdown.l45',\n",
    "                                'max_tp', 'min_tp', 'rain', 'humidity',\n",
    "                                'day_of_the_week',  'trend'])\n",
    "\n",
    "            output.append(self.OLS_models)\n",
    "\n",
    "        if \"ARX\" in self.desired_model_groups:\n",
    "            # MG2T and MG3T: Introducing lagged dependent terms to the previous model specifications.\n",
    "\n",
    "            self.ARX_models = self.get_all_possible_combinations(\n",
    "                model_group=['ARX'],\n",
    "                MG_ar_orders=self.ar_orders,\n",
    "                MG_regressors=['log_new_people_vaccinated_per_capita',  'delta_cases_per_capita',\n",
    "                                'delta_deaths_per_capita_United Kingdom', 'delta_deaths_per_capita_Germany', 'delta_deaths_per_capita_France',\n",
    "                                'full_lockdown', 'full_lockdown.l30', 'full_lockdown.l45',\n",
    "                                'max_tp', 'min_tp', 'rain', 'humidity',\n",
    "                                'day_of_the_week', 'trend'])\n",
    "                                \n",
    "            output.append(self.ARX_models)\n",
    "\n",
    "        # Returning the functional sets to be deployed.\n",
    "        return output"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "ar_orders = np.arange(1, 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "models = ModelGroupSpecs(ar_orders, ['AR', \"OLS\", \"ARX\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "model_groups = models.create_functional_sets()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "model_groups"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[('AR', 1, None), ('AR', 2, None)],\n",
       " [('OLS', None, 'log_new_people_vaccinated_per_capita'),\n",
       "  ('OLS', None, 'delta_cases_per_capita'),\n",
       "  ('OLS', None, 'delta_cases_per_capita_United_Kingdom'),\n",
       "  ('OLS', None, 'delta_cases_per_capita_Germany'),\n",
       "  ('OLS', None, 'delta_cases_per_capita_France'),\n",
       "  ('OLS', None, 'delta_deaths_per_capita_United Kingdom'),\n",
       "  ('OLS', None, 'delta_deaths_per_capita_Germany'),\n",
       "  ('OLS', None, 'delta_deaths_per_capita_France'),\n",
       "  ('OLS', None, 'full_lockdown'),\n",
       "  ('OLS', None, 'full_lockdown.l30'),\n",
       "  ('OLS', None, 'full_lockdown.l45'),\n",
       "  ('OLS', None, 'max_tp'),\n",
       "  ('OLS', None, 'min_tp'),\n",
       "  ('OLS', None, 'rain'),\n",
       "  ('OLS', None, 'humidity'),\n",
       "  ('OLS', None, 'day_of_the_week'),\n",
       "  ('OLS', None, 'trend')],\n",
       " [('ARX', 1, 'log_new_people_vaccinated_per_capita'),\n",
       "  ('ARX', 1, 'delta_cases_per_capita'),\n",
       "  ('ARX', 1, 'delta_deaths_per_capita_United Kingdom'),\n",
       "  ('ARX', 1, 'delta_deaths_per_capita_Germany'),\n",
       "  ('ARX', 1, 'delta_deaths_per_capita_France'),\n",
       "  ('ARX', 1, 'full_lockdown'),\n",
       "  ('ARX', 1, 'full_lockdown.l30'),\n",
       "  ('ARX', 1, 'full_lockdown.l45'),\n",
       "  ('ARX', 1, 'max_tp'),\n",
       "  ('ARX', 1, 'min_tp'),\n",
       "  ('ARX', 1, 'rain'),\n",
       "  ('ARX', 1, 'humidity'),\n",
       "  ('ARX', 1, 'day_of_the_week'),\n",
       "  ('ARX', 1, 'trend'),\n",
       "  ('ARX', 2, 'log_new_people_vaccinated_per_capita'),\n",
       "  ('ARX', 2, 'delta_cases_per_capita'),\n",
       "  ('ARX', 2, 'delta_deaths_per_capita_United Kingdom'),\n",
       "  ('ARX', 2, 'delta_deaths_per_capita_Germany'),\n",
       "  ('ARX', 2, 'delta_deaths_per_capita_France'),\n",
       "  ('ARX', 2, 'full_lockdown'),\n",
       "  ('ARX', 2, 'full_lockdown.l30'),\n",
       "  ('ARX', 2, 'full_lockdown.l45'),\n",
       "  ('ARX', 2, 'max_tp'),\n",
       "  ('ARX', 2, 'min_tp'),\n",
       "  ('ARX', 2, 'rain'),\n",
       "  ('ARX', 2, 'humidity'),\n",
       "  ('ARX', 2, 'day_of_the_week'),\n",
       "  ('ARX', 2, 'trend')]]"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "combined = pd.read_csv(\"Data/Combined_Dataset.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from tqdm import tqdm\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def training_and_forecasting(k,\n",
    "                data_and_time,\n",
    "                y_var,\n",
    "                functional_sets,\n",
    "                ):\n",
    "    \"\"\"\n",
    "    Input specifications (by example):\n",
    "        forecast_horizon=3, known as $k$ in our main work.\n",
    "        Other choices include 1, 2, 5, 10.\n",
    "        data_and_time. This should be a pandas dataframe including (1) a time index, (2) the dependent variable of interest\n",
    "        and (3) exogenous variables.\n",
    "        functional_sets. This is the set of all models to be trained. It is obtained via the ModelGroupSpecs class.\n",
    "        ar_orders = [1, 2,..., 10]. For MG2T and MG3T these order run to up to 5.\n",
    "        window_sizes = [22, 63, 126, 252].\n",
    "\n",
    "    Return:\n",
    "        - The set of forecasts produced by all fixed model groups.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1 (a): Defining our indexes for training and testing.\n",
    "    T_max = len(data_and_time) - 1\n",
    "    T_train = np.arange(100, T_max+1)\n",
    "    # Logistics\n",
    "    # Creation of H_tilda\n",
    "\n",
    "    forecast_df = create_forecast_df(functional_sets)\n",
    "\n",
    "    # Step 2 (a): Training the models and making a forecast.\n",
    "    for t in tqdm(T_train, leave=True, position=0):\n",
    "\n",
    "        forecasts_t = [t]\n",
    "\n",
    "        for functional_set in functional_sets:\n",
    "            for model_group, ar_order, regressor in functional_set:\n",
    "\n",
    "                w = t-2\n",
    "\n",
    "                if model_group == 'AR':\n",
    "\n",
    "                    forecasts = train_and_forecast_MG1(data = data_and_time[y_var],\n",
    "                                                       t = t,\n",
    "                                                       w =w,\n",
    "                                                       ar_order = ar_order,\n",
    "                                                       k = k)\n",
    "\n",
    "                elif model_group == 'OLS':\n",
    "\n",
    "                    forecasts = train_and_forecast_MG2N(regressor = data_and_time[regressor],\n",
    "                                                        dep_data = data_and_time[y_var],\n",
    "                                                        t = t,\n",
    "                                                        w = w,\n",
    "                                                        k = k)\n",
    "\n",
    "                elif model_group == 'ARX':\n",
    "\n",
    "                    forecasts = train_and_forecast_MG_2T_3T(dep_data=data_and_time[y_var],\n",
    "                                                            regressor=data_and_time[regressor],\n",
    "                                                            t=t,\n",
    "                                                            w = w,\n",
    "                                                            ar_order=ar_order,\n",
    "                                                            k=k)\n",
    "\n",
    "\n",
    "        \n",
    "                forecasts_t.append(float(forecasts))\n",
    "        \n",
    "        forecast_df.loc[t] = forecasts_t\n",
    "\n",
    "    return forecast_df\n",
    "\n",
    "\n",
    "##### Individual Training and forecasting Functions See Appendix 1.5 and 1.6 of the pseudo-algorithm #####\n",
    "\n",
    "def train_and_forecast_MG1(data, t, w, ar_order, k):\n",
    "\n",
    "    windowed_data = np.array(data[(t - w + 1): (t + 1)])\n",
    "\n",
    "    model = ARIMA(windowed_data, order=(\n",
    "        ar_order, 0, 0), trend = 'c').fit()\n",
    "\n",
    "    forecasts = model.forecast(k)[::-1]\n",
    "\n",
    "    return forecasts[-1]\n",
    "\n",
    "\n",
    "def train_and_forecast_MG2N(regressor, dep_data, t, w, k):\n",
    "\n",
    "    forecasts = []\n",
    "\n",
    "    indep_train_data = np.array(regressor[max(0, t - w - k + 1): (t - k + 1)])\n",
    "\n",
    "    indep_train_data = sm.add_constant(indep_train_data)\n",
    "\n",
    "    dep_train_data = np.array(dep_data[max(0, t - w + 1): (t + 1)])\n",
    "\n",
    "    model = sm.OLS(dep_train_data,\n",
    "                   indep_train_data).fit()\n",
    "\n",
    "    for i in range(k):\n",
    "\n",
    "        forecasts.append(model.params[0] + model.params[1] * regressor[t - i])\n",
    "\n",
    "    return forecasts[-1]\n",
    "\n",
    "\n",
    "def train_and_forecast_MG_2T_3T(dep_data, regressor, t, w, ar_order, k):\n",
    "\n",
    "    indep_train_data = vectorise_indep_variables(dep_to_be_lagged = dep_data,\n",
    "                                                 exog=regressor,\n",
    "                                                 t=t,\n",
    "                                                 ar_order=ar_order,\n",
    "                                                 w=w,\n",
    "                                                 k=k)\n",
    "\n",
    "    dep_train_data = dep_data[max(0, t - w + 1): (t + 1)]\n",
    "\n",
    "    model = sm.OLS(dep_train_data, sm.add_constant(indep_train_data)).fit()\n",
    "\n",
    "    forecasts = forecast_with_lags(\n",
    "        model.params, dep_data, regressor, t, ar_order, k)\n",
    "\n",
    "    return forecasts\n",
    "\n",
    "\n",
    "def vectorise_indep_variables(dep_to_be_lagged, exog, t, ar_order, w, k):\n",
    "\n",
    "    lagged_dep_data = list(dep_to_be_lagged[(t - w - ar_order): (t)])\n",
    "\n",
    "    lagged_dep_data = lagged_dep_data[::-1]\n",
    "\n",
    "    lagged_p = [lagged_dep_data[j: j+ar_order]\n",
    "                for j in range(0, len(lagged_dep_data) - ar_order)]\n",
    "\n",
    "    lagged_p = np.array(lagged_p[:: -1])\n",
    "\n",
    "    exog = np.array(exog[(t - w - k + 1): (t - k + 1)])\n",
    "\n",
    "    vectorised_training_data = np.array(\n",
    "        [np.append(lagged_p[j], [exog[j]]) for j in range(0, len(exog))])\n",
    "\n",
    "    return vectorised_training_data\n",
    "\n",
    "\n",
    "def forecast_with_lags(model_params, dep_to_be_lagged, regressor, t, ar_order, k):\n",
    "\n",
    "    forecasts = []\n",
    "\n",
    "    lagged_y = np.array(\n",
    "        dep_to_be_lagged[(t - ar_order + 1): (t + 1)])\n",
    "    lagged_x = np.array(regressor[(t - k + 1): (t + 1)])\n",
    "\n",
    "    for tau in range(0, k):  # tau = 0 means 1-step-ahead.\n",
    "\n",
    "        observations_under_consideration = [[1]]\n",
    "\n",
    "        if len(forecasts) > 0:\n",
    "            observations_under_consideration.append(\n",
    "                forecasts[::-1][:ar_order])\n",
    "\n",
    "        observations_under_consideration.append(lagged_y[tau:][::-1])\n",
    "\n",
    "        observations_under_consideration.append(lagged_x[tau].flatten())\n",
    "\n",
    "        observations_under_consideration = np.concatenate(\n",
    "            observations_under_consideration).ravel()\n",
    "\n",
    "        forecasts.append(np.dot(np.array(observations_under_consideration),\n",
    "                                np.array(model_params)))\n",
    "\n",
    "    return forecasts[-1]\n",
    "\n",
    "\n",
    "#### Helper Functions ####\n",
    "\n",
    "def naming(model_group, ar_order, regressor):\n",
    "    return f\"{model_group}, AR{ar_order}, Regressor = {regressor}\"\n",
    "\n",
    "def create_forecast_df(functional_sets):\n",
    "    headers = ['t']\n",
    "    for functional_set in functional_sets:\n",
    "        for model_group, ar_order, regressor in functional_set:\n",
    "            headers.append(naming(model_group, ar_order, regressor))\n",
    "    forecast_df = pd.DataFrame(columns=[headers])\n",
    "    return forecast_df\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "forecasts = training_and_forecasting(1, combined, \"delta_deaths_per_capita\", model_groups)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 511/511 [04:32<00:00,  1.88it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "del forecasts['t']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "import numpy as np\n",
    "from pandas import read_csv as rc\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "\n",
    "def adaptive_learning(k,\n",
    "                      data_and_time,\n",
    "                      forecast_df,\n",
    "                      y_var,\n",
    "                      functional_sets,\n",
    "                      specification_learning\n",
    "                      ):\n",
    "\n",
    "    # Loading\n",
    "    AL_specification = specification_learning[0]\n",
    "\n",
    "    if AL_specification == 'EN':\n",
    "        v, p, Lambda = specification_learning[1]\n",
    "        lambda_vector = [Lambda**(v-i) for i in range(0, v)]\n",
    "\n",
    "    # Step 1: Housekeeping.\n",
    "    # Step 1 (a):\n",
    "    T_max = len(data_and_time) - 1\n",
    "\n",
    "    # Step 1 (b): Define the training index.\n",
    "    T_train = np.arange(100,\n",
    "                        T_max)\n",
    "                        \n",
    "\n",
    "    # Step 1 (c): Define the testing index.\n",
    "    T_test = np.arange(100 + v,\n",
    "                       T_max)\n",
    "\n",
    "    forecast_errors = create_H_tilda_dict(functional_sets)\n",
    "\n",
    "    # Step 2.\n",
    "    for t in tqdm(T_train, leave=True, position=0):\n",
    "        for model_name in functional_sets:\n",
    "\n",
    "            # Step 2 (a)(i): Obtain a forecast according to the model.\n",
    "\n",
    "            forecast_value = forecast_df.loc[t, model_name]\n",
    "\n",
    "        \n",
    "            # Step 2 (a)(ii): Obtain the forecasting error.\n",
    "            val_data = data_and_time.loc[t+k, y_var]\n",
    "\n",
    "            error_t_plus_k = abs(forecast_value - val_data)\n",
    "\n",
    "            forecast_errors[model_name][t+k] = error_t_plus_k\n",
    "\n",
    "    # Step 3. Implement AL via the designated option.\n",
    "\n",
    "    if AL_specification == \"EN\":\n",
    "        optimal_models = {}\n",
    "        y_star_dict = {}\n",
    "\n",
    "    # Step 3(c).\n",
    "    for t in tqdm(T_test, leave=True, position=0):\n",
    "\n",
    "        # Step 3 (c)(i): Declare T_tilda (the adaptive learning lookback window).\n",
    "        T_tilda = np.arange(t - v + 1, t + 1)\n",
    "\n",
    "        if AL_specification == \"EN\":\n",
    "\n",
    "            y_star, h_star = regular_AL(T_tilda=T_tilda,\n",
    "                                        t=t,\n",
    "                                        AL_specification=\"EN\",\n",
    "                                        functional_sets=functional_sets,\n",
    "                                        forecast_df=forecast_df,\n",
    "                                        forecast_errors=forecast_errors,\n",
    "                                        lambda_vector=lambda_vector,\n",
    "                                        p=p)\n",
    "\n",
    "\n",
    "\n",
    "            # Step 3 (d): Save this h star (best model) and make an associated forecast\n",
    "            optimal_models.update({t: h_star})\n",
    "            y_star_dict.update({t: y_star})\n",
    "\n",
    "\n",
    "        Output = [\n",
    "            functional_sets,\n",
    "            [T_test, T_train],\n",
    "            optimal_models,\n",
    "            y_star_dict\n",
    "        ]\n",
    "\n",
    "    return Output\n",
    "\n",
    "\n",
    "def regular_AL(T_tilda,\n",
    "               t,\n",
    "               AL_specification,\n",
    "               functional_sets,\n",
    "               forecast_df,\n",
    "               forecast_errors,\n",
    "               lambda_vector,\n",
    "               p):\n",
    "\n",
    "    # Step 3 (c)(ii).\n",
    "    loss_by_model = {}\n",
    "\n",
    "    for model_name in functional_sets:\n",
    "\n",
    "        # Step 3 (c)(ii)(A): Collect the array of forecasting errors\n",
    "        # over the adaptive learning lookback window and evaluate the loss over the period T tilda.\n",
    "        errors = [forecast_errors[model_name][tau] for tau in T_tilda]\n",
    "\n",
    "        if AL_specification == \"EN\":\n",
    "            loss = exponential_learning(errors=errors, lam=lambda_vector, p=p)\n",
    "\n",
    "        loss_by_model.update({model_name: loss})\n",
    "\n",
    "    # Step 3 (c)(ii)(B): Find the argmin of the loss function for h in H over the period Tilda.\n",
    "    h_star = min(loss_by_model, key=loss_by_model.get)\n",
    "\n",
    "    # Step 3 (c)(ii)(C): Save this h star (best model) and make the associated forecast.\n",
    "    y_star = forecast_df.loc[t, h_star]\n",
    "\n",
    "    return float(y_star), h_star\n",
    "\n",
    "\n",
    "###### 1.7.1 Exponential-Norm Learning Function ######\n",
    "\n",
    "def exponential_learning(errors, lam, p):\n",
    "    e_vector = np.array(errors)\n",
    "    lambda_vector = np.array(lam)\n",
    "    loss = np.dot(np.squeeze(np.power(e_vector, p)), lambda_vector)\n",
    "    return loss\n",
    "\n",
    "##### Helper Functions ####\n",
    "\n",
    "\n",
    "def create_H_tilda_dict(H):\n",
    "    H_tilda = {}\n",
    "    for model in H:\n",
    "        H_tilda.update({model: {}})\n",
    "    return H_tilda\n",
    "\n",
    "\n",
    "def create_value_dict(H):\n",
    "    H_tilda = {}\n",
    "    for model in H:\n",
    "        H_tilda.update({model: 0})\n",
    "    return H_tilda\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "AL = adaptive_learning(1, \n",
    "                       combined, \n",
    "                       forecast_df = forecasts, \n",
    "                       y_var = 'delta_deaths_per_capita', \n",
    "                       functional_sets = forecasts.columns, \n",
    "                       specification_learning= [\"EN\", [100, 2, 0.96]])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 510/510 [00:04<00:00, 106.03it/s]\n",
      "100%|██████████| 410/410 [00:01<00:00, 333.21it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "preds = pd.DataFrame(AL[3], index = [i for i in AL[3].keys()]).iloc[0, :]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "pd.DataFrame(list(AL[2].values())).value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "OLS, ARNone, Regressor = delta_deaths_per_capita_United Kingdom    151\n",
       "ARX, AR1, Regressor = delta_deaths_per_capita_United Kingdom        79\n",
       "ARX, AR1, Regressor = delta_deaths_per_capita_France                40\n",
       "OLS, ARNone, Regressor = full_lockdown                              22\n",
       "ARX, AR2, Regressor = day_of_the_week                               20\n",
       "ARX, AR2, Regressor = delta_deaths_per_capita_United Kingdom        17\n",
       "ARX, AR1, Regressor = day_of_the_week                               16\n",
       "AR, AR2, Regressor = None                                           14\n",
       "ARX, AR1, Regressor = full_lockdown.l45                             12\n",
       "OLS, ARNone, Regressor = delta_deaths_per_capita_France             11\n",
       "ARX, AR2, Regressor = delta_deaths_per_capita_France                 9\n",
       "AR, AR1, Regressor = None                                            6\n",
       "ARX, AR2, Regressor = full_lockdown.l45                              4\n",
       "ARX, AR1, Regressor = full_lockdown                                  4\n",
       "ARX, AR1, Regressor = humidity                                       2\n",
       "ARX, AR2, Regressor = delta_deaths_per_capita_Germany                1\n",
       "ARX, AR1, Regressor = max_tp                                         1\n",
       "OLS, ARNone, Regressor = full_lockdown.l45                           1\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "first_half = pd.DataFrame(list(AL[2].values())).loc[:len(list(AL[2].values()))/2]\n",
    "second_half = pd.DataFrame(list(AL[2].values())).loc[len(list(AL[2].values()))/2:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "first_half.value_counts().to_latex(\"first_half.txt\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "first_half.value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "OLS, ARNone, Regressor = delta_deaths_per_capita_United Kingdom    48\n",
       "ARX, AR1, Regressor = delta_deaths_per_capita_France               35\n",
       "ARX, AR1, Regressor = delta_deaths_per_capita_United Kingdom       29\n",
       "ARX, AR2, Regressor = day_of_the_week                              20\n",
       "ARX, AR1, Regressor = day_of_the_week                              16\n",
       "AR, AR2, Regressor = None                                          14\n",
       "ARX, AR1, Regressor = full_lockdown.l45                            12\n",
       "ARX, AR2, Regressor = delta_deaths_per_capita_France                9\n",
       "AR, AR1, Regressor = None                                           6\n",
       "ARX, AR1, Regressor = full_lockdown                                 4\n",
       "ARX, AR2, Regressor = delta_deaths_per_capita_United Kingdom        4\n",
       "ARX, AR2, Regressor = full_lockdown.l45                             4\n",
       "ARX, AR1, Regressor = humidity                                      2\n",
       "ARX, AR1, Regressor = max_tp                                        1\n",
       "ARX, AR2, Regressor = delta_deaths_per_capita_Germany               1\n",
       "OLS, ARNone, Regressor = full_lockdown.l45                          1\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "second_half.value_counts().to_latex(\"second_half.txt\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "second_half.value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "OLS, ARNone, Regressor = delta_deaths_per_capita_United Kingdom    103\n",
       "ARX, AR1, Regressor = delta_deaths_per_capita_United Kingdom        51\n",
       "OLS, ARNone, Regressor = full_lockdown                              22\n",
       "ARX, AR2, Regressor = delta_deaths_per_capita_United Kingdom        13\n",
       "OLS, ARNone, Regressor = delta_deaths_per_capita_France             11\n",
       "ARX, AR1, Regressor = delta_deaths_per_capita_France                 5\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "4e7a58de4b7e505b0e82f0adcc21bb9621f61c2c1c0bdf66b0394a18a0c298a7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}